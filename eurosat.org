* About this project
This project uses Convolutional Neural Networks to classify
images. Our dataset is called EuroSAT and there is a paper about it
called "EuroSAT: A Novel Dataset and Deep Learning Benchmark for Land
Use and Land Cover Classification" [[https://github.com/phelber/EuroSAT][here]]. Basically we will copy the
method of the paper.

We will use the pretrained base ResNet-50 and other goodies from
Keras like callbacks and data augmentation layers. We also use
Matplotlib for basic plots and NumPy for data structures.

An outline of the program (about 150 lines of Python code) :
- Import statements.
- Define some functions for repetitive things.
- ~main()~ function that has the main thought process of training the
  model.

The EuroSAT dataset is a collection of 27.000 satellite images taken
above European countries. Every image is 64 by 64 pixels and every
pixel can be red, green, or blue (RGB). So every data point has shape
(64, 64, 3). We chose this dataset because of the small size.

There are ten possible output classes :
- Industrial Building.
- Residential Building.
- Annual Crop.
- Permanent Crop.
- River.
- Sea and Lake.
- Herbaceous Vegetation.
- Highway.
- Pasture.
- Forest.

Our job is to find the right class for every image.
* Imports
Let's import some tools for image classification.

~resnet50~ is our pretrained base model. It is powerful and saves us
from designing the architecture ourselves.
#+begin_src python
from keras.applications import resnet50
#+end_src
~EarlyStopping~ and ~ReduceLROnPlateau~ are essentially forms of
regularization.
#+begin_src python
from keras.callbacks import EarlyStopping, ReduceLROnPlateau
#+end_src
We don't really know what ~HeNormal~ does but it stays.
#+begin_src python
from keras.initializers import HeNormal
#+end_src
In order :
- ~Dense~ layers are basic Neural Network layers and we use one as a
  hat to sit on top of the ~resnet50~ base model. Another one is used
  to pick one of eleven classes.
- ~Dropout~ is used as regularization to make the training process
  smooth.
- ~Flatten~ is used before ~Dropout~ and classification by the ~Dense~
  layer.
- ~IntegerLookup~ is for one-hot-encoding the outputs which are
  categorical.
- ~RandomBrightness~, ~RandomConstrast~, and ~RandomFlip~ are data
  augmentation layers used to make more data than what we are given.
#+begin_src python
from keras.layers import (Dense, Dropout, Flatten, IntegerLookup,
			  RandomBrightness, RandomContrast,
			  RandomFlip)
#+end_src
~CategoricalCrossentropy~ is our loss function and
~CategoricalAccuracy~ is our metric which we track. We use the ~Adam~
optimizer but any optimizer can be used.
#+begin_src python
from keras.losses import CategoricalCrossentropy
from keras.metrics import CategoricalAccuracy
from keras.optimizers import Adam
from keras import models
#+end_src
Occasionally we will need to round a ~float~ down to an ~int~.
#+begin_src python
from math import floor
#+end_src
Let's get ~keras~, ~matplotlib~, ~numpy~, ~tensorflow~, and
~tensorflow_datasets~.
#+begin_src python
import keras
import matplotlib.pyplot as plt
import numpy as np
import tensorflow as tf
import tensorflow_datasets as tfds
#+end_src
* Functions
This function builds the model by applying the three data augmentation
layers ~RandomFlip~, ~RandomBrightness~, and ~RandomContrast~ to the
input. Then we feed the input into the base which is ~resnet50~. We
~Flatten~ and ~Dropout~, then train our own ~Dense~ layer with 512
units and get the output by a second ~Dense~ layer with eleven
possible outputs. There are eleven outputs and not ten because the
~IntegerLookup~ layer considers the possibility that the output may be
outside the vocabulary.

The ~param_dict~ argument will be defined later as a dictionary of
tunable parameters. At the moment there is only one parameter : ~"lr"~
or learning rate.
#+begin_src python
  def build_model(param_dict, base):
      input = keras.Input(shape=(64, 64, 3))

      x = RandomFlip()(input)
      x = RandomBrightness(0.5)(x)
      x = RandomContrast(0.5)(x)
      x = base(x)
      x = Flatten()(x)
      x = Dropout(0.1)(x)
      x = Dense(512, activation="relu", kernel_initializer=HeNormal())(x)

      output = Dense(11, activation="softmax")(x)

      model = keras.Model(input, output)
      model.compile(optimizer=Adam(learning_rate=param_dict["lr"]),
		    loss=CategoricalCrossentropy(),
		    metrics=[CategoricalAccuracy()])

      return model
#+end_src
This function returns the callbacks. The ~EarlyStopping~ callback will
stop the training process if ~monitor~ does not improve after
~patience~ epochs. The ~ReduceLROnPlateau~ callback will multiply the
learning rate by ~factor~ if ~monitor~ does not improve after
~patience~ epochs. These are basic but effective.
#+begin_src python
  def get_callbacks():
      early_stop_loss = EarlyStopping(monitor="loss", patience=8)
      reduce_lr_plateau = ReduceLROnPlateau(monitor="loss", factor=0.9,
					    patience=4)

      return [early_stop_loss, reduce_lr_plateau]
#+end_src
This function loads the EuroSAT dataset as one big batch and gives us
the ~images~ and ~labels~ separately.
#+begin_src python
  def get_eurosat_dataset():
      images, labels = tfds.load("eurosat", split="train",
				 as_supervised=True, batch_size=-1)

      return images, labels
#+end_src
This function takes a number and returns the ~param_dict~ mentioned
earlier with parameters taking a random value within +-10% of the
original value. We also print the values for something to look at
while training.
#+begin_src python
  def get_random_numbers(middle):
      rng = np.random.default_rng()
      param_dict = {"lr": middle * (0.9 + 0.2 * rng.random())}
      print(param_dict)

      return param_dict
#+end_src
This function makes the plots and labels the axes. We want to know
about loss and accuracy during training and validation. There is a
~start_at~ variable which tells the plot to skip the first few
values because the first few values are usually much smaller than the
later ones.
#+begin_src python
  def plot(history):
      acc = history.history["categorical_accuracy"]
      loss = history.history["loss"]
      val_acc = history.history["val_categorical_accuracy"]
      val_loss = history.history["val_loss"]

      epochs = range(len(loss))
      start_at = 2

      plt.plot(epochs[start_at:], loss[start_at:], "ob",
	       label="Training Loss")
      plt.plot(epochs[start_at:], val_loss[start_at:], "xb",
	       label="Validation Loss")
      plt.title("Loss")
      plt.xlabel("Epochs")
      plt.ylabel("Loss")
      plt.legend()
      plt.show()

      plt.plot(epochs[start_at:], acc[start_at:], "ob",
	       label="Training Accuracy")
      plt.plot(epochs[start_at:], val_acc[start_at:], "xb",
	       label="Validation Accuracy")
      plt.title("Accuracy")
      plt.xlabel("Epochs")
      plt.ylabel("Accuracy")
      plt.legend()
      plt.show()

#+end_src
* Main
The ~main()~ function calls the shots.
#+begin_src python
def main():
#+end_src
We begin by loading the dataset.
#+begin_src python
images, labels = get_eurosat_dataset()
#+end_src
We get a list of percentile indices for easy dataset splitting later.
#+begin_src python
  percents = [floor(np.shape(images)[0] * (i / 100.0))
	      for i in range(100)]
#+end_src
Let's load the ~resnet50~ base and freeze the layers. We will unfreeze
the layers later as the paper says to do.
#+begin_src python
  base = resnet50.ResNet50(include_top=False, input_shape=(64, 64, 3))
  base.trainable = False
#+end_src
Vocabulary for the ten output classes.
#+begin_src python
  label_vocab = [i for i in range(10)]
#+end_src
One-hot-encode the labels using the vocabulary from above.
#+begin_src python
  one_hot_encode_layer = IntegerLookup(vocabulary=label_vocab,
				       output_mode="one_hot")
  encoded_labels = one_hot_encode_layer(labels)
#+end_src
We split the dataset into an 80-20 (Training-Test) split.
#+begin_src python
  split = 80
  train_images = images[:percents[split]]
  test_images = images[percents[split]:]
  train_labels = encoded_labels[:percents[split]]
  test_labels = encoded_labels[percents[split]:]
#+end_src
Load the callbacks.
#+begin_src python
  callbacks = get_callbacks()
#+end_src
Train our two ~Dense~ layers first. We use a 10% validation split so
the dataset now has a 72-8-20 (Training-Validation-Test) split. We
shuffle to keep things standard casino operating procedure.
#+begin_src python
  print('Fitting model.')
  model = build_model(get_random_numbers(0.01), base)
  history = model.fit(train_images, train_labels, batch_size=128,
		      callbacks=callbacks, epochs=64, shuffle=True,
		      validation_split=0.1, verbose=1)

  plot(history)
#+end_src
Next we set ~base.trainable~ to ~True~ to unfreeze the base layers but
this time we train with a much smaller learning rate. We don't want
any large disruptions here.
#+begin_src python
  print("Fine-tuning.")
  base.trainable = True
  model = build_model(get_random_numbers(0.0001), base)
  history = model.fit(train_images, train_labels, batch_size=128,
		      callbacks=callbacks, epochs=128, shuffle=True,
		      validation_split=0.1, verbose=1)

  plot(history)
#+end_src
Call the ~evaluate()~ function with ~test_images~ and ~test_labels~ to
see how we did. Hopefully our final results here are very close to our
earlier validation results.
#+begin_src python
  print("Predicting.")
  model.evaluate(test_images, test_labels, verbose=1)
#+end_src
These lines go at the end outside of the ~main()~ function's
indentation level.
#+begin_src python
  if __name__ == "__main__":
      main()
#+end_src
* Conclusions
At this point after running the program a few times we see that our
model gets about 96% test accuracy. We may do some further analysis
with a confusion matrix or something else to figure out which images
we are still misclassifying. The benchmark from the paper is about
98.5% test accuracy with an 80-20 split using the ~resnet50~ base. We
are satisfied for now with 96%.
* References
- Helber, Patrick and others, "EuroSAT: A Novel Dataset and Deep
  Learning Benchmark for Land Use and Land Cover Classification",
  /IEEE Journal of Selected Topics in Applied Earth Observations and
  Remote Sensing/, 2017, [[https://github.com/phelber/EuroSAT][EuroSAT GitHub]].
* To Do Later
- Add links.
